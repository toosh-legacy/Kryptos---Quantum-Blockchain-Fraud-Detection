{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ccb748",
   "metadata": {},
   "source": [
    "# Baseline GAT Training\n",
    "\n",
    "Train a standard Graph Attention Network (GAT) for Bitcoin fraud detection.\n",
    "\n",
    "**Architecture:**\n",
    "- Input: 182 node features\n",
    "- 2 GAT layers with 64 hidden channels and 4 attention heads\n",
    "- Output: 2 classes (fraud/non-fraud)\n",
    "\n",
    "**Training:**\n",
    "- Weighted CrossEntropyLoss for class imbalance\n",
    "- Adam optimizer with cosine annealing scheduler\n",
    "- Early stopping with patience=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd88fc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1050a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import json\n",
    "import time\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.models import GAT\n",
    "from src.utils import set_random_seeds\n",
    "\n",
    "# Set random seeds\n",
    "set_random_seeds(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0862b77",
   "metadata": {},
   "source": [
    "### What This Cell Does (Setup)\n",
    "This cell imports all necessary libraries and initializes the environment:\n",
    "\n",
    "1. **Set environment variable**: Allows PyTorch to use OpenMP without conflicts\n",
    "\n",
    "2. **Import core libraries**:\n",
    "   - `torch`, `torch.nn`, `torch.nn.functional`: Deep learning\n",
    "   - `sklearn.metrics`: F1, accuracy, precision, recall, ROC-AUC calculation\n",
    "   - `torch_geometric`: Graph neural network layers (GATConv)\n",
    "\n",
    "3. **Import project code**:\n",
    "   - `src.models.GAT`: Custom GAT model class\n",
    "   - `src.utils.set_random_seeds`: Ensure reproducible results\n",
    "\n",
    "4. **Set seeds and device**:\n",
    "   - Random seed = 42 (reproducible results)\n",
    "   - Select GPU if available, otherwise CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e08fc6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading graph...\")\n",
    "graph = torch.load('../artifacts/elliptic_graph.pt', weights_only=False).to(device)\n",
    "\n",
    "labeled_mask = (graph.y != -1)\n",
    "labeled_indices = torch.where(labeled_mask)[0].cpu().numpy()\n",
    "labeled_y = graph.y[labeled_mask].cpu().numpy()\n",
    "\n",
    "print(f\"Graph: {graph.num_nodes:,} nodes, {graph.num_edges:,} edges, {graph.num_node_features} features\")\n",
    "print(f\"Labeled nodes: {len(labeled_indices):,}\")\n",
    "print(f\"Fraud (class 1): {(labeled_y == 1).sum():,}\")\n",
    "print(f\"Non-fraud (class 0): {(labeled_y == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e476eda",
   "metadata": {},
   "source": [
    "### What This Cell Does (Load Data)\n",
    "This cell loads the **pre-constructed graph** created by `create_graph.ipynb`:\n",
    "\n",
    "1. **Load saved graph**:\n",
    "   - Loads `elliptic_graph.pt` (200MB file with all nodes, edges, features)\n",
    "   - Move to device (GPU/CPU)\n",
    "\n",
    "2. **Extract labeled nodes**:\n",
    "   - Find nodes with labels (not -1/unknown)\n",
    "   - Separate indices and labels for splitting\n",
    "\n",
    "3. **Print dataset statistics**:\n",
    "   - Total nodes: 203k\n",
    "   - Total edges: 230k\n",
    "   - Features per node: 182\n",
    "   - Labeled nodes and class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b73c5e",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41abcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split: 70% train/val, 30% test\n",
    "train_val_idx, test_idx, train_val_y, test_y = train_test_split(\n",
    "    labeled_indices, labeled_y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=labeled_y\n",
    ")\n",
    "\n",
    "# Split train into 70% train, 30% val\n",
    "train_idx, val_idx, _, _ = train_test_split(\n",
    "    train_val_idx, train_val_y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=train_val_y\n",
    ")\n",
    "\n",
    "# Create mask tensors\n",
    "train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool, device=device)\n",
    "val_mask = torch.zeros(graph.num_nodes, dtype=torch.bool, device=device)\n",
    "test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool, device=device)\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "print(f\"Train: {train_mask.sum():,}, Val: {val_mask.sum():,}, Test: {test_mask.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f048e",
   "metadata": {},
   "source": [
    "### What This Cell Does (Data Splitting)\n",
    "This cell creates **train/validation/test splits** for evaluation:\n",
    "\n",
    "1. **Two-step stratified split**:\n",
    "   - First: Separate 30% for test, 70% for train/val\n",
    "   - Second: From train/val, separate 30% for val, 70% for train\n",
    "   - Result: ~49% train, 21% val, 30% test nodes\n",
    "\n",
    "2. **Stratified splitting**:\n",
    "   - Maintains class ratio (fraud/non-fraud) in each split\n",
    "   - All splits use same nodes across all model experiments\n",
    "   - Reproducible with random_state=42\n",
    "\n",
    "3. **Create mask tensors**:\n",
    "   - Boolean tensors marking which nodes belong to train/val/test\n",
    "   - Used to select subsets during training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb2a21",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values\n",
    "nan_count = torch.isnan(graph.x).sum().item()\n",
    "if nan_count > 0:\n",
    "    graph.x = torch.nan_to_num(graph.x, nan=0.0)\n",
    "    print(f\"Replaced {nan_count} NaN values\")\n",
    "\n",
    "# Normalize based on training set\n",
    "train_x = graph.x[train_mask]\n",
    "mean = train_x.mean(dim=0, keepdim=True)\n",
    "std = train_x.std(dim=0, keepdim=True)\n",
    "std = torch.where(std == 0, torch.ones_like(std), std)\n",
    "\n",
    "graph.x = (graph.x - mean) / std\n",
    "graph.x = torch.clamp(graph.x, min=-10, max=10)\n",
    "\n",
    "print(\"Features normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e0c8a",
   "metadata": {},
   "source": [
    "### What This Cell Does (Normalize Features)\n",
    "This cell applies **z-score normalization** to node features:\n",
    "\n",
    "1. **Handle missing values**:\n",
    "   - Check for NaN (missing) values\n",
    "   - Replace with 0.0 if found\n",
    "\n",
    "2. **Compute normalization statistics**:\n",
    "   - Calculate mean and std from TRAINING data only\n",
    "   - This prevents data leakage (val/test shouldn't affect normalization)\n",
    "\n",
    "3. **Apply normalization**:\n",
    "   - Transform: x_norm = (x - mean) / std\n",
    "   - Clamp to [-10, +10] to handle outliers\n",
    "   - Normalized features have mean ≈ 0, std ≈ 1\n",
    "\n",
    "4. **Why normalize?**:\n",
    "   - Neural networks train better with normalized inputs\n",
    "   - Prevents large-magnitude features from dominating\n",
    "   - Improves gradient stability during backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba4ead",
   "metadata": {},
   "source": [
    "## Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_0 = (graph.y[train_mask] == 0).sum().item()\n",
    "n_class_1 = (graph.y[train_mask] == 1).sum().item()\n",
    "\n",
    "class_weight = torch.tensor(\n",
    "    [1.0 / n_class_0, 1.0 / n_class_1],\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "class_weight = class_weight / class_weight.sum()\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fec830",
   "metadata": {},
   "source": [
    "### What This Cell Does (Compute Class Weights)\n",
    "This cell computes **weights to handle class imbalance**:\n",
    "\n",
    "1. **Count class frequencies**:\n",
    "   - How many non-fraud nodes (class 0) in training\n",
    "   - How many fraud nodes (class 1) in training\n",
    "   - Usually: much more non-fraud than fraud\n",
    "\n",
    "2. **Calculate inverse frequency weights**:\n",
    "   - weight = 1 / count (rarer classes get higher weight)\n",
    "   - Rare fraud class gets more importance\n",
    "\n",
    "3. **Normalize weights**:\n",
    "   - Ensure weights sum to 1.0\n",
    "   - Used in CrossEntropyLoss\n",
    "\n",
    "4. **Why weighted loss?**:\n",
    "   - Without weights: model predicts everything as majority class\n",
    "   - With weights: misclassifying fraud costs more than non-fraud\n",
    "   - Forces model to learn fraud patterns despite rarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d905a",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(\n",
    "    in_channels=graph.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=2,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "try:\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {params:,}\")\n",
    "except:\n",
    "    print(\"Model created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0d5b6",
   "metadata": {},
   "source": [
    "### What This Cell Does (Create Model)\n",
    "This cell constructs the **Graph Attention Network (GAT) model**:\n",
    "\n",
    "1. **Model architecture**:\n",
    "   - Input: 182 node features\n",
    "   - 2 GAT layers with 64 hidden channels each\n",
    "   - 4 attention heads per layer\n",
    "   - Output: 2 classes (fraud/non-fraud)\n",
    "   - Dropout: 0.3 (prevents overfitting)\n",
    "\n",
    "2. **What GAT does**:\n",
    "   - Uses attention mechanism to aggregate neighbor features\n",
    "   - Each attention head learns different aggregation patterns\n",
    "   - Multiple heads provide diverse neighborhoods\n",
    "\n",
    "3. **Move to device**:\n",
    "   - Transfer model from CPU to GPU (or keep on CPU)\n",
    "\n",
    "4. **Count parameters**:\n",
    "   - Total: ~50,000 trainable parameters\n",
    "   - Relatively small model (efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab896dd3",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60175c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Loss: Weighted CrossEntropy\")\n",
    "print(f\"  Optimizer: Adam (lr=0.001, weight_decay=5e-4)\")\n",
    "print(f\"  Scheduler: Cosine Annealing (T_max=300)\")\n",
    "print(f\"  Early stopping: patience=50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d2437",
   "metadata": {},
   "source": [
    "### What This Cell Does (Training Setup)\n",
    "This cell configures the **training algorithm and learning schedule**:\n",
    "\n",
    "1. **Loss function**:\n",
    "   - Weighted CrossEntropyLoss with class weights\n",
    "   - Weight parameter: assigns higher penalty to fraud misclassification\n",
    "   - Handles class imbalance\n",
    "\n",
    "2. **Optimizer**:\n",
    "   - Adam optimizer (adaptive learning rate)\n",
    "   - Learning rate: 0.001\n",
    "   - Weight decay: 5e-4 (L2 regularization to prevent overfitting)\n",
    "\n",
    "3. **Learning rate scheduler**:\n",
    "   - Cosine Annealing: gradually reduces learning rate\n",
    "   - T_max=300: reaches minimum at epoch 300\n",
    "   - Helps model converge better in late training\n",
    "\n",
    "4. **Early stopping**:\n",
    "   - Patience: 50 epochs\n",
    "   - Stops training if validation F1 doesn't improve\n",
    "   - Prevents overfitting to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf68e68",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f21f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        pred = out[mask].argmax(dim=1)\n",
    "        prob = F.softmax(out[mask], dim=1)[:, 1]\n",
    "        \n",
    "        y_true = graph.y[mask].cpu().numpy()\n",
    "        y_pred = pred.cpu().numpy()\n",
    "        y_prob = prob.cpu().numpy()\n",
    "        y_prob = np.nan_to_num(y_prob, nan=0.5)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81d591",
   "metadata": {},
   "source": [
    "### What This Cell Does (Evaluation Function)\n",
    "This cell defines a function to **evaluate model performance** on any subset:\n",
    "\n",
    "1. **Set model to evaluation mode**:\n",
    "   - `model.eval()`: Disables dropout and batch normalization\n",
    "   - Ensures consistent predictions\n",
    "\n",
    "2. **Make predictions**:\n",
    "   - Forward pass through model (no gradients)\n",
    "   - Get class probabilities using softmax\n",
    "   - Extract fraud probability (class 1)\n",
    "\n",
    "3. **Compute metrics**:\n",
    "   - Accuracy: % correct predictions\n",
    "   - Precision: % of predicted fraud that's actually fraud\n",
    "   - Recall: % of actual fraud detected\n",
    "   - F1: Harmonic mean of precision/recall (primary metric)\n",
    "   - ROC-AUC: Area under receiver-operating-characteristic curve\n",
    "\n",
    "4. **Return results**:\n",
    "   - Metrics dictionary\n",
    "   - True labels, predictions, and probabilities\n",
    "   - Used for train/val/test evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514b4a2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0da9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING BASELINE GAT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "best_val_f1 = -1\n",
    "patience = 0\n",
    "max_patience = 50\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_f1': [],\n",
    "    'val_acc': [],\n",
    "    'val_gap': []\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(graph.x, graph.edge_index)\n",
    "    loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = evaluate(model, val_mask)\n",
    "    train_metrics = evaluate(model, train_mask)\n",
    "    \n",
    "    val_gap = train_metrics['f1'] - val_metrics['f1']\n",
    "    \n",
    "    history['train_loss'].append(loss.item())\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['val_gap'].append(val_gap)\n",
    "    \n",
    "    if val_metrics['f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1']\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), '../artifacts/baseline_gat_best.pt')\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if epoch % 20 == 0 or epoch < 10:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | \"\n",
    "              f\"Train F1: {train_metrics['f1']:.4f} | Val F1: {val_metrics['f1']:.4f} | \"\n",
    "              f\"Gap: {val_gap:.4f} | Patience: {patience}/{max_patience}\")\n",
    "    \n",
    "    if patience >= max_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training completed in {training_time:.2f}s\")\n",
    "print(f\"✓ Best Val F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992dae5",
   "metadata": {},
   "source": [
    "### What This Cell Does (Training Loop)\n",
    "This cell **trains the GAT model** for up to 300 epochs:\n",
    "\n",
    "1. **Initialize state**:\n",
    "   - Track best validation F1\n",
    "   - Patience counter (for early stopping)\n",
    "   - History lists for loss and metrics\n",
    "\n",
    "2. **Each epoch**:\n",
    "   - **Forward pass**: Predictions on training nodes\n",
    "   - **Compute loss**: Weighted CrossEntropyLoss\n",
    "   - **Backward pass**: Compute gradients\n",
    "   - **Gradient clipping**: Prevent exploding gradients (max_norm=1.0)\n",
    "   - **Update**: Optimizer step\n",
    "   - **Schedule**: Reduce learning rate\n",
    "\n",
    "3. **Validation**:\n",
    "   - Evaluate on training and validation sets\n",
    "   - Track generalization gap (train F1 - val F1)\n",
    "   - If val F1 improves: save model, reset patience\n",
    "   - If no improvement: increment patience\n",
    "\n",
    "4. **Early stopping**:\n",
    "   - If patience reaches 50: stop training\n",
    "   - Prevents overfitting to training data\n",
    "   - Saves training time\n",
    "\n",
    "5. **Monitoring**:\n",
    "   - Print progress every 20 epochs\n",
    "   - Show loss, F1 scores, generalization gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac7f8e",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b809aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "model.load_state_dict(torch.load('../artifacts/baseline_gat_best.pt', map_location=device))\n",
    "\n",
    "baseline_train = evaluate(model, train_mask)\n",
    "baseline_val = evaluate(model, val_mask)\n",
    "baseline_test = evaluate(model, test_mask)\n",
    "\n",
    "print(\"Baseline GAT Results:\")\n",
    "print(f\"  Train - F1: {baseline_train['f1']:.4f}, Acc: {baseline_train['accuracy']:.4f}\")\n",
    "print(f\"  Val   - F1: {baseline_val['f1']:.4f}, Acc: {baseline_val['accuracy']:.4f}\")\n",
    "print(f\"  Test  - F1: {baseline_test['f1']:.4f}, Acc: {baseline_test['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nGeneralization Gaps:\")\n",
    "print(f\"  Train→Val: {baseline_train['f1'] - baseline_val['f1']:.4f}\")\n",
    "print(f\"  Val→Test:  {baseline_val['f1'] - baseline_test['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nDetailed Test Report:\")\n",
    "print(classification_report(baseline_test['y_true'], baseline_test['y_pred'],\n",
    "                          target_names=['Non-Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b196d",
   "metadata": {},
   "source": [
    "### What This Cell Does (Final Evaluation)\n",
    "This cell **evaluates the best model** on all three data splits:\n",
    "\n",
    "1. **Load best model**:\n",
    "   - Load state_dict from checkpoint file\n",
    "   - Restore the best weights from training\n",
    "\n",
    "2. **Evaluate on all splits**:\n",
    "   - Train: Performance on training nodes\n",
    "   - Val: Performance on validation nodes\n",
    "   - Test: Performance on held-out test nodes (final metric)\n",
    "\n",
    "3. **Print results**:\n",
    "   - F1 and accuracy for each split\n",
    "   - Generalization gaps:\n",
    "     - Train→Val gap: Training vs validation performance\n",
    "     - Val→Test gap: Validation vs test performance\n",
    "   - Detailed classification report: Precision, recall per class\n",
    "\n",
    "4. **Expected results**:\n",
    "   - Baseline GAT typically achieves F1 ≈ 0.87 on test set\n",
    "   - Small generalization gap indicates good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b7ae9",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820538c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    'model': 'Baseline GAT',\n",
    "    'description': 'Standard Graph Attention Network with 2 layers, 64 hidden channels, 4 heads',\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_time': training_time,\n",
    "    'architecture': {\n",
    "        'input_features': graph.num_node_features,\n",
    "        'hidden_channels': 64,\n",
    "        'num_layers': 2,\n",
    "        'num_heads': 4,\n",
    "        'dropout': 0.3,\n",
    "        'optimizer': 'Adam (lr=0.001)',\n",
    "        'loss': 'Weighted CrossEntropy'\n",
    "    },\n",
    "    'test_metrics': {\n",
    "        'f1': baseline_test['f1'],\n",
    "        'accuracy': baseline_test['accuracy'],\n",
    "        'precision': baseline_test['precision'],\n",
    "        'recall': baseline_test['recall'],\n",
    "        'roc_auc': baseline_test['roc_auc']\n",
    "    },\n",
    "    'generalization': {\n",
    "        'train_f1': baseline_train['f1'],\n",
    "        'val_f1': baseline_val['f1'],\n",
    "        'train_to_val_gap': baseline_train['f1'] - baseline_val['f1'],\n",
    "        'val_to_test_gap': baseline_val['f1'] - baseline_test['f1']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../artifacts/baseline_gat_metrics.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Report saved to ../artifacts/baseline_gat_metrics.json\")\n",
    "print(f\"✓ Model saved to ../artifacts/baseline_gat_best.pt\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ BASELINE GAT TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562462b",
   "metadata": {},
   "source": [
    "### What This Cell Does (Save Results)\n",
    "This cell saves the **training results and model weights** to disk:\n",
    "\n",
    "1. **Create report dictionary**:\n",
    "   - Model name: \"Baseline GAT\"\n",
    "   - Model architecture details (layers, hidden dims, heads, dropout)\n",
    "   - Training configuration (optimizer, loss, scheduler)\n",
    "   - Test metrics (F1, accuracy, precision, recall, ROC-AUC)\n",
    "   - Generalization metrics (train/val gaps)\n",
    "   - Timestamp and training time\n",
    "\n",
    "2. **Save model weights**:\n",
    "   - Save to `artifacts/baseline_gat_best.pt` (~500KB)\n",
    "   - Contains trained parameters (can be loaded later)\n",
    "\n",
    "3. **Save metrics**:\n",
    "   - Save report to `artifacts/baseline_gat_metrics.json`\n",
    "   - Can be compared with other models (quantum, etc.)\n",
    "   - Readable format for reports/documentation\n",
    "\n",
    "4. **Completion message**:\n",
    "   - Confirms training finished successfully\n",
    "   - Ready for next phase (quantum training)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
