{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b12d43",
   "metadata": {},
   "source": [
    "# Train Quantum-Enhanced GAT Model\n",
    "\n",
    "This notebook trains a GAT model on quantum-transformed features.\n",
    "\n",
    "## Overview\n",
    "1. Load quantum graph with expanded features (182â†’364 dimensions)\n",
    "2. Initialize GAT model with quantum features\n",
    "3. Train with early stopping (or load existing model)\n",
    "4. Evaluate and save results\n",
    "\n",
    "## Important Notes\n",
    "- **Estimated time:** 10-20 minutes for training (or instant if model exists)\n",
    "- If you encounter errors, try: **Kernel â†’ Restart Kernel** from the menu\n",
    "- The notebook will automatically load pre-trained model if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192abb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any cached src imports\n",
    "import sys\n",
    "if 'src' in sys.modules:\n",
    "    del sys.modules['src']\n",
    "for key in list(sys.modules.keys()):\n",
    "    if key.startswith('src.'):\n",
    "        del sys.modules[key]\n",
    "print(\"âœ“ Cleared cached imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Add project root to sys.path\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.models import GAT\n",
    "from src.config import QUANTUM_MODEL_CONFIG, TRAINING_CONFIG, ARTIFACTS_DIR, ARTIFACT_FILES\n",
    "from src.utils import set_random_seeds, get_device\n",
    "from src.train_utils import FocalLoss, compute_class_weights\n",
    "\n",
    "# Set random seeds\n",
    "set_random_seeds(TRAINING_CONFIG['random_seed'])\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380092a6",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum graph: Data(x=[203769, 364], edge_index=[2, 672479], y=[203769], timestep=[203769], labeled_mask=[203769], unlabeled_mask=[203769])\n",
      "Feature dimension: 364\n",
      "\n",
      "Creating train/val/test splits...\n",
      "\n",
      "Train: 27938, Val: 9313, Test: 9313\n",
      "Parameters: 94,470\n"
     ]
    }
   ],
   "source": [
    "graph_path = ARTIFACTS_DIR / ARTIFACT_FILES['quantum_graph']\n",
    "data = torch.load(graph_path, weights_only=False).to(device)\n",
    "print(f\"Quantum graph: {data}\")\n",
    "print(f\"Feature dimension: {data.num_node_features}\")\n",
    "\n",
    "# Check if masks exist, otherwise create them\n",
    "if not hasattr(data, 'train_mask'):\n",
    "    print(\"\\nCreating train/val/test splits...\")\n",
    "    labeled_indices = torch.where(data.labeled_mask)[0].cpu().numpy()\n",
    "    labeled_y = data.y[data.labeled_mask].cpu().numpy()\n",
    "    \n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        labeled_indices, \n",
    "        test_size=TRAINING_CONFIG['train_test_split'],\n",
    "        random_state=TRAINING_CONFIG['random_seed'],\n",
    "        stratify=labeled_y\n",
    "    )\n",
    "    \n",
    "    train_val_y = data.y[train_val_idx].cpu().numpy()\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        train_val_idx,\n",
    "        test_size=TRAINING_CONFIG['train_val_split'],\n",
    "        random_state=TRAINING_CONFIG['random_seed'],\n",
    "        stratify=train_val_y\n",
    "    )\n",
    "    \n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "print(f\"\\nTrain: {data.train_mask.sum()}, Val: {data.val_mask.sum()}, Test: {data.test_mask.sum()}\")\n",
    "\n",
    "# Check class distribution\n",
    "train_labels = data.y[data.train_mask]\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"  Class 0 (Licit): {(train_labels == 0).sum()}\")\n",
    "print(f\"  Class 1 (Illicit): {(train_labels == 1).sum()}\")\n",
    "print(f\"  Imbalance ratio: {(train_labels == 0).sum().item() / (train_labels == 1).sum().item():.2f}:1\")\n",
    "\n",
    "# Initialize model with quantum-expanded features (use QUANTUM_MODEL_CONFIG for better capacity)\n",
    "print(f\"\\nðŸ”§ Initializing GAT model with quantum configuration...\")\n",
    "model = GAT(\n",
    "    in_channels=data.num_node_features,\n",
    "    hidden_channels=QUANTUM_MODEL_CONFIG['hidden_channels'],\n",
    "    out_channels=QUANTUM_MODEL_CONFIG['out_channels'],\n",
    "    num_heads=QUANTUM_MODEL_CONFIG['num_heads'],\n",
    "    num_layers=QUANTUM_MODEL_CONFIG['num_layers'],\n",
    "    dropout=QUANTUM_MODEL_CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=TRAINING_CONFIG['learning_rate'], \n",
    "    weight_decay=TRAINING_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# CRITICAL FIX: Address class imbalance with weighted loss\n",
    "if TRAINING_CONFIG['use_class_weights']:\n",
    "    class_weights = compute_class_weights(train_labels, num_classes=2).to(device)\n",
    "    print(f\"\\nâš–ï¸  Class weights: {class_weights.cpu().numpy()}\")\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    print(\"âœ“ Using weighted CrossEntropyLoss\")\n",
    "elif TRAINING_CONFIG['use_focal_loss']:\n",
    "    criterion = FocalLoss(\n",
    "        alpha=TRAINING_CONFIG['focal_alpha'], \n",
    "        gamma=TRAINING_CONFIG['focal_gamma']\n",
    "    )\n",
    "    print(f\"âœ“ Using Focal Loss (alpha={TRAINING_CONFIG['focal_alpha']}, gamma={TRAINING_CONFIG['focal_gamma']})\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"âš ï¸  Using standard CrossEntropyLoss (no class balancing)\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if TRAINING_CONFIG['lr_scheduler']:\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',  # Maximize F1 score\n",
    "        factor=TRAINING_CONFIG['lr_factor'],\n",
    "        patience=TRAINING_CONFIG['lr_patience'],\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"âœ“ Using ReduceLROnPlateau scheduler (patience={TRAINING_CONFIG['lr_patience']})\")\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "print(\"\\nâœ“ Model, optimizer, and loss function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e6210",
   "metadata": {},
   "source": [
    "## 3. Define Training & Evaluation Functions\n",
    "\n",
    "Training function with gradient descent and evaluation function with standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Training & Evaluation Functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    \n",
    "    # CRITICAL FIX: Gradient clipping for stable training\n",
    "    if TRAINING_CONFIG.get('clip_grad_norm'):\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), \n",
    "            max_norm=TRAINING_CONFIG['clip_grad_norm']\n",
    "        )\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    prob = F.softmax(out[mask], dim=1)[:, 1]\n",
    "\n",
    "    y_true = data.y[mask].cpu().numpy()\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_prob = prob.cpu().numpy()\n",
    "\n",
    "    # Handle NaN values in predictions\n",
    "    if np.isnan(y_prob).any():\n",
    "        print(f\"Warning: Found {np.isnan(y_prob).sum()} NaN values in predictions, replacing with 0.5\")\n",
    "        y_prob = np.nan_to_num(y_prob, nan=0.5)\n",
    "    \n",
    "    # Ensure no infinite values\n",
    "    if np.isinf(y_prob).any():\n",
    "        print(f\"Warning: Found {np.isinf(y_prob).sum()} Inf values in predictions, replacing with 0.5\")\n",
    "        y_prob = np.nan_to_num(y_prob, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "\n",
    "    # Compute metrics with error handling\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        print(f\"Warning: Could not compute ROC AUC: {e}\")\n",
    "        roc_auc = 0.5\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c64f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quantum GAT...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m     11\u001b[39m     loss = train_epoch()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     val_metrics = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(loss)\n\u001b[32m     15\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mval_metrics\u001b[39m\u001b[33m'\u001b[39m].append(val_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tusha\\Documents\\UT_Dallas\\ACM_SP26\\imple2\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(mask)\u001b[39m\n\u001b[32m     18\u001b[39m y_pred = pred.cpu().numpy()\n\u001b[32m     19\u001b[39m y_prob = prob.cpu().numpy()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: accuracy_score(y_true, y_pred),\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m: precision_score(y_true, y_pred, zero_division=\u001b[32m0\u001b[39m),\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m: recall_score(y_true, y_pred, zero_division=\u001b[32m0\u001b[39m),\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m: f1_score(y_true, y_pred, zero_division=\u001b[32m0\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mroc_auc\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tusha\\Documents\\UT_Dallas\\ACM_SP26\\imple2\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tusha\\Documents\\UT_Dallas\\ACM_SP26\\imple2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:683\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    681\u001b[39m y_type = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    682\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m y_score = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    686\u001b[39m     y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score.shape[\u001b[32m1\u001b[39m] > \u001b[32m2\u001b[39m\n\u001b[32m    687\u001b[39m ):\n\u001b[32m    688\u001b[39m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[32m    689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr != \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tusha\\Documents\\UT_Dallas\\ACM_SP26\\imple2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1069\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1070\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1071\u001b[39m     )\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1083\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tusha\\Documents\\UT_Dallas\\ACM_SP26\\imple2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:133\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tusha\\Documents\\UT_Dallas\\ACM_SP26\\imple2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:182\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    168\u001b[39m     msg_err += (\n\u001b[32m    169\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Cell 4: Train Quantum GAT Model (or load existing)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "model_save_path = ARTIFACTS_DIR / ARTIFACT_FILES['quantum_model']\n",
    "\n",
    "# Check if model already exists\n",
    "if Path(model_save_path).exists():\n",
    "    print(f\"âš ï¸  Model already exists at {model_save_path}\")\n",
    "    print(\"   Loading existing model instead of retraining...\")\n",
    "    print(\"   (Delete the file and restart kernel to retrain from scratch)\\n\")\n",
    "    \n",
    "    # Load existing model\n",
    "    checkpoint = torch.load(model_save_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_val_f1 = checkpoint['val_f1']\n",
    "    training_time = 0  # Not trained in this session\n",
    "    \n",
    "    print(f\"âœ“ Loaded model from epoch {checkpoint['epoch']} with Val F1: {best_val_f1:.4f}\")\n",
    "    print(\"âœ“ Model ready for evaluation\")\n",
    "else:\n",
    "    print(\"ðŸš€ Training quantum GAT model with all improvements...\")\n",
    "    print(\"   - Weighted CrossEntropyLoss for class imbalance\")\n",
    "    print(\"   - Increased model capacity (128 hidden, 6 heads, 3 layers)\")\n",
    "    print(\"   - Gradient clipping for stability\")\n",
    "    print(\"   - Learning rate scheduling\")\n",
    "    print(\"   - Learnable quantum features\\n\")\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], \n",
    "        'val_acc': [], \n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': [], \n",
    "        'val_auc': []\n",
    "    }\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    EPOCHS = TRAINING_CONFIG['epochs']\n",
    "    PATIENCE = TRAINING_CONFIG['patience']\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        loss = train_epoch()\n",
    "        val_metrics = evaluate(data.val_mask)\n",
    "        \n",
    "        history['train_loss'].append(loss)\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_auc'].append(val_metrics['roc_auc'])\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Val F1: {val_metrics['f1']:.4f} | \"\n",
    "                  f\"Val Prec: {val_metrics['precision']:.4f} | Val Rec: {val_metrics['recall']:.4f} | \"\n",
    "                  f\"Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_metrics['f1'])\n",
    "        \n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': best_val_f1,\n",
    "                'val_metrics': val_metrics,\n",
    "                'config': QUANTUM_MODEL_CONFIG\n",
    "            }, model_save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"\\nâœ“ Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save training history\n",
    "    results = {\n",
    "        'training_time': training_time,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'total_epochs': epoch,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    results_path = ARTIFACTS_DIR / 'quantum_training_results.json'\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Training complete in {training_time:.2f}s\")\n",
    "    print(f\"âœ… Best Val F1: {best_val_f1:.4f}\")\n",
    "    print(f\"âœ… Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd47958",
   "metadata": {},
   "source": [
    "## 4. Train Quantum GAT Model\n",
    "\n",
    "Train with early stopping based on validation F1 score. If model already exists, it will be loaded instead of retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46550585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint = torch.load(model_save_path, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ“ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate(data.test_mask)\n",
    "\n",
    "print(\"\\nðŸ“Š Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = ARTIFACTS_DIR / ARTIFACT_FILES['quantum_metrics']\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced92c2",
   "metadata": {},
   "source": [
    "## 5. Load Best Model & Evaluate\n",
    "\n",
    "Load the best model checkpoint and evaluate on all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b03c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Save Metrics to JSON\n",
    "# Get training time from checkpoint or use 0 if model was pre-loaded\n",
    "try:\n",
    "    training_time_value = training_time\n",
    "except NameError:\n",
    "    training_time_value = 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'model_type': 'GAT_Quantum',\n",
    "    'training_time': training_time_value,\n",
    "    'best_epoch': checkpoint['epoch'],\n",
    "    'performance': {\n",
    "        'train': train_metrics,\n",
    "        'val': val_metrics,\n",
    "        'test': test_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = ARTIFACTS_DIR / ARTIFACT_FILES['quantum_metrics']\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Metrics saved to {metrics_path}\")\n",
    "print(\"\\nâœ… Quantum GAT training complete!\")\n",
    "print(\"ðŸ“Š Proceed to 07_eval_quantum.ipynb to compare with baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666aff7",
   "metadata": {},
   "source": [
    "## 6. Save Metrics to JSON\n",
    "\n",
    "Save all performance metrics for comparison in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ae08a",
   "metadata": {},
   "source": [
    "## 6. Save Metrics to JSON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
